{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11994585,"sourceType":"datasetVersion","datasetId":7544671,"isSourceIdPinned":false}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"NLP_Assignment_QA_Full","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Download resource","metadata":{"id":"b_AuYMpcX-bd"}},{"cell_type":"code","source":"import kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:06.834543Z","iopub.execute_input":"2025-05-29T18:52:06.835257Z","iopub.status.idle":"2025-05-29T18:52:06.838709Z","shell.execute_reply.started":"2025-05-29T18:52:06.835231Z","shell.execute_reply":"2025-05-29T18:52:06.837816Z"},"id":"5PsY23e9X-be"},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Download library","metadata":{"id":"6aygIs2CX-bf"}},{"cell_type":"code","source":"!pip install -q datasets\n!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:06.839906Z","iopub.execute_input":"2025-05-29T18:52:06.840167Z","iopub.status.idle":"2025-05-29T18:52:13.258570Z","shell.execute_reply.started":"2025-05-29T18:52:06.840142Z","shell.execute_reply":"2025-05-29T18:52:13.257771Z"},"id":"6aPmLOMMX-bf","outputId":"2dd60336-4449-4f50-b95b-877eccd211a9"},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# import pandas as pd\nimport datasets\nimport numpy as np\nimport re,string\nimport torch\nimport torch.nn as nn\nimport math\nimport nltk\nimport pandas as pd\ndevice = torch.device(\"cuda\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:13.260149Z","iopub.execute_input":"2025-05-29T18:52:13.260458Z","iopub.status.idle":"2025-05-29T18:52:13.265940Z","shell.execute_reply.started":"2025-05-29T18:52:13.260427Z","shell.execute_reply":"2025-05-29T18:52:13.265079Z"},"id":"9QM0vOboX-bg"},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Load dataset","metadata":{"id":"r6l2BUr4X-bg"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\n\ndataset_train = load_dataset(\"rajpurkar/squad\", split=\"train\")\ndataset_val = load_dataset(\"rajpurkar/squad\", split=\"validation\")\n\ntrain_df = pd.DataFrame(dataset_train)\nval_df = pd.DataFrame(dataset_val)\n\ntrain_df['answer_text'] = train_df['answers'].apply(lambda x: x['text'][0] if x['text'] else None)\ntrain_df['answer_start'] = train_df['answers'].apply(lambda x: x['answer_start'][0] if x['answer_start'] else None)\ntrain_df = train_df.drop(columns=['answers'])\ntrain_df = train_df[['id', 'title', 'context', 'question', 'answer_text', 'answer_start']]\n\n\n\nval_df['answer_text'] = val_df['answers'].apply(lambda x: x['text'][0] if x['text'] else None)\nval_df['answer_start'] = val_df['answers'].apply(lambda x: x['answer_start'][0] if x['answer_start'] else None)\nval_df = val_df.drop(columns=['answers'])\nval_df = val_df[['id', 'title', 'context', 'question', 'answer_text', 'answer_start']]\n\n\ntrain_df = train_df.head(200)  # For testing purposes\nval_df = val_df.head(200)      # For testing purposes\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:13.266767Z","iopub.execute_input":"2025-05-29T18:52:13.267058Z","iopub.status.idle":"2025-05-29T18:52:22.768378Z","shell.execute_reply.started":"2025-05-29T18:52:13.267032Z","shell.execute_reply":"2025-05-29T18:52:22.767780Z"},"jupyter":{"source_hidden":true},"id":"YiqOeRjdX-bg","outputId":"a9edf61e-6ef8-4dc1-b131-b9daa8f799cc","colab":{"referenced_widgets":["82474d62ab644d39ac55febc4d49b862","adb8e445afbb481aaf64b628fb9d3162","9106c2d07789466485594bfbf57baae8","efa13a9cfca044e98601973e1c559d9a","1ac488462cad4ad2bfb837c8206f2f74"]}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## Prepare `Datasets` object","metadata":{"id":"zEyVxNKCX-bh"}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom transformers import logging\nlogging.set_verbosity_error()\nfrom torch.optim import AdamW\nfrom peft import LoraConfig, get_peft_model\n\nclass QADataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=384):\n        # Tokenizer is the model's tokenizer, it should come with the model like the GPT2 one\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length # Maximum tokens in a sequence\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        question, context = row['question'], row['context']\n        answer_text, answer_start = row['answer_text'], int(row['answer_start'])\n        inputs = self.tokenizer(\n            question, context, # Sentence A, Sentence B\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_offsets_mapping=True,\n            return_tensors=\"pt\"\n        )\n\n        #          Dataset\n        #  field            | Semantic meaning\n        #  answer_text      | Paris\n        #  answer_start     | 32 (character index inside the context used for the answer)\n        # Aftertokenization -> Loses the characters, only token remains\n        # Offset mapping map the token to it's indexing in the context text\n        offsets = inputs.pop(\"offset_mapping\")[0]\n        input_ids = inputs[\"input_ids\"][0] # Token IDs\n        attention_mask = inputs[\"attention_mask\"][0] # Attention mask (1s and 0s, 0 = Ignore this token (padding))\n\n        #  Some moded needs the id of each token to be a 0 for question, 1 for contex\n        token_type_ids = inputs.get(\"token_type_ids\")\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids[0]\n\n        # Find the start and end token indices that cover the  text used for asnwering the question.\n        start_char, end_char = answer_start, answer_start + len(answer_text)\n        start_token = 0\n        end_token = 0\n        for i, (o_start, o_end) in enumerate(offsets.tolist()):\n            # \"If the answer start character is inside this token → mark this token as start_token.\"\n            if o_start <= start_char < o_end:\n                start_token = i\n            # \"If the answer end character falls inside this token → mark this token as end_token.\"\n            if o_start < end_char <= o_end:\n                end_token = i\n\n        item = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n            \"end_positions\": torch.tensor(end_token, dtype=torch.long),\n            \"answer_text\":      answer_text,\n        }\n        if token_type_ids is not None:\n            item[\"token_type_ids\"] = token_type_ids\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:22.769737Z","iopub.execute_input":"2025-05-29T18:52:22.770017Z","iopub.status.idle":"2025-05-29T18:52:22.779649Z","shell.execute_reply.started":"2025-05-29T18:52:22.770000Z","shell.execute_reply":"2025-05-29T18:52:22.779044Z"},"jupyter":{"source_hidden":true},"id":"-rbZ_HiSX-bh","outputId":"c25d9fa0-ad77-4031-acba-07d5da5d077e"},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# 2. Build abstract function","metadata":{"id":"WdTzTv8vX-bh"}},{"cell_type":"markdown","source":"## 2.1 Training function","metadata":{"id":"HViBnz3VX-bh"}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom math import ceil\n\nBATCH_SIZE = 16\nLEARNING_RATE = 2e-4\nEPOCHS = 1\n\ndef _suffix(name: str):\n    return name if name else 'full'\n\n# Training and saving\ndef train_and_save(model, tokenizer, train_loader, strategy, model_name, val_df):\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n    best_loss = float('inf')\n    suffix = _suffix(strategy)\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        batch_iterator = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n\n        for i, batch in enumerate(batch_iterator, 1):\n            #inputs = {k: v.to(device) for k, v in batch.items()}\n            inputs = {}\n            for k, v in batch.items():\n                if isinstance(v, torch.Tensor):\n                    inputs[k] = v.to(device)\n            # batch[\"answer_text\"] remains a Python list of strings for debugging\n            # batch is already the example we’re training on\n            input_ids = batch[\"input_ids\"][0]\n            s, e      = batch[\"start_positions\"][0].item(), batch[\"end_positions\"][0].item()\n            #print(\"Decoded span:  \", tokenizer.decode(input_ids[s:e+1], skip_special_tokens=True))\n            #print(\"Ground truth:  \", batch[\"answer_text\"][0])\n\n            loss = model(**inputs).loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            total_loss += loss.item()\n            avg_loss_so_far = total_loss / i\n            batch_iterator.set_postfix(loss=avg_loss_so_far)\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch} - Average loss: {avg_loss:.4f}\")\n\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            out_dir = f\"best_{model_name.replace('/', '_')}_{suffix}\"\n            if strategy == 'prompt':\n                # Save only the adapter for prompt tuning\n                model.save_pretrained(out_dir)\n            else:\n                model.save_pretrained(out_dir)\n                tokenizer.save_pretrained(out_dir)\n            print(f\"Saved best checkpoint to {out_dir}\")\n\n        val_results = evaluate_model(\n            model_name, strategy, val_df, metric, device\n        )\n        print(\n            f\"Epoch {epoch} — Validation → \"\n            f\"EM: {val_results['exact_match']:.2f}%, \"\n            f\"F1: {val_results['f1']:.2f}%\"\n        )\n\n\n\ndef save_model(model, tokenizer, out_dir):\n    os.makedirs(out_dir, exist_ok=True)\n    model.save_pretrained(out_dir)\n    tokenizer.save_pretrained(out_dir)\n    print(f\"Saved best model to {out_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:22.780297Z","iopub.execute_input":"2025-05-29T18:52:22.780464Z","iopub.status.idle":"2025-05-29T18:52:22.803619Z","shell.execute_reply.started":"2025-05-29T18:52:22.780450Z","shell.execute_reply":"2025-05-29T18:52:22.802919Z"},"jupyter":{"source_hidden":true},"id":"MvCah-LMX-bi"},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## 2.2 Model preparation function","metadata":{"id":"YxxzT0suX-bi"}},{"cell_type":"code","source":"from peft import LoraConfig, PromptTuningConfig, get_peft_model,PrefixTuningConfig, AdaLoraConfig,PromptEncoderConfig\ndef prepare_model(model_name: str, strategy: str):\n    # Huggging face will takes care of everything and add the Question and Answering head to the model\n    # The head will output a tupple of 2 values\n    # The score that this token is the start of the answer\n    #The score that this token is the end of the answer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    if strategy == 'lora':\n\n        if model_name == \"distilbert-base-uncased\":\n            target_modules = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\", \"lin1\", \"lin2\"]\n        else:\n            target_modules = [\n                \"attention.self.query\",\n                \"attention.self.key\",\n                \"attention.self.value\",\n                \"attention.output.dense\",\n                \"intermediate.dense\",\n            ]\n\n        cfg = LoraConfig(\n            task_type=\"QUESTION_ANS\",\n            r=8,\n            lora_alpha=32,\n            lora_dropout=0.1,\n            target_modules=target_modules,\n            bias=\"none\",\n        )\n        model = get_peft_model(model, cfg)\n        for n,p in model.named_parameters():\n            if n.startswith(\"base_model.model.qa_outputs\"):\n                p.requires_grad = True\n    elif strategy == 'adalora':\n\n        if model_name == \"distilbert-base-uncased\":\n            target_modules = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\", \"lin1\", \"lin2\"]\n        else:\n            target_modules = [\n                \"attention.self.query\",\n                \"attention.self.key\",\n                \"attention.self.value\",\n                \"attention.output.dense\",\n                \"intermediate.dense\",\n            ]\n        # ---- AdaLoRA ----\n        ada_cfg = AdaLoraConfig(\n            task_type=\"QUESTION_ANS\",\n            init_r=4,            # start LoRA adapters at rank=4\n            target_r=8,          # grow/prune them toward rank=8\n            total_step=train_steps,\n            target_modules=target_modules,\n        )\n        model = get_peft_model(model, ada_cfg)\n        # still allow QA head adaptation\n        for n,p in model.named_parameters():\n            if n.startswith(\"base_model.model.qa_outputs\"):\n                p.requires_grad = True\n\n        # immediately report your new trainable‐param count\n    elif strategy == 'bitfit':\n        for n, p in model.named_parameters(): p.requires_grad=False\n        for n, p in model.named_parameters():\n            if 'bias' in n: p.requires_grad=True\n        # For empty strategy string, we perform full fine-tuning of all parameters\n    # else: strategy == '' -> full fine-tuning (no changes)\n    elif strategy == 'prompt':\n        cfg_m      = model.config\n        specials   = tokenizer.num_special_tokens_to_add(pair=True)  # =3 for QA\n        free_spots = cfg_m.max_position_embeddings - MAX_LEN - specials\n        num_virtual = min(200, free_spots)\n        print(num_virtual)\n        if 'distilbert' in model_name:\n            # DistilBERT needs explicit sizing\n            prompt_cfg = PromptTuningConfig(\n                task_type='QUESTION_ANS',\n                num_virtual_tokens=num_virtual,\n                num_layers=cfg_m.n_layers,\n                token_dim=cfg_m.dim,\n                num_attention_heads=cfg_m.n_heads,\n            )\n        else:\n            # BERT / RoBERTa / MiniLM can auto-detect layers & dims\n            prompt_cfg = PromptTuningConfig(\n                task_type='QUESTION_ANS',\n                num_virtual_tokens=num_virtual,\n            )\n\n\n        model = get_peft_model(model, prompt_cfg)\n\n        #for n, p in model.named_parameters():\n        #    if 'bias' in n: p.requires_grad=True\n\n        for n,p in model.named_parameters() :\n            if n.startswith(\"base_model.model.qa_outputs\") or n.startswith(\"prompt_encoder\"):\n                p.requires_grad = True\n    model.to(device)\n    return model, tokenizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:22.804419Z","iopub.execute_input":"2025-05-29T18:52:22.804619Z","iopub.status.idle":"2025-05-29T18:52:22.818221Z","shell.execute_reply.started":"2025-05-29T18:52:22.804604Z","shell.execute_reply":"2025-05-29T18:52:22.817661Z"},"jupyter":{"source_hidden":true},"id":"rR0HsH2RX-bi"},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 2.3 Model evaluation function","metadata":{"id":"YIeKWL_RX-bi"}},{"cell_type":"code","source":"MAX_LEN = 384\ndef evaluate_model(model_name, strategy, df, metric, device, max_len=MAX_LEN):\n    # This used for evaluating the model on the validation set after each epoch\n    # Evaluating a pretrain model will be discussed in anothersection below\n    suffix = _suffix(strategy)\n    out_dir = f\"best_{model_name.replace('/', '_')}_{suffix}\"\n\n    if strategy == 'prompt':\n        # Load base model and adapter separately for prompt tuning\n        base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        config = PromptTuningConfig.from_pretrained(out_dir)\n        model = get_peft_model(base_model, config)\n    else:\n        model = AutoModelForQuestionAnswering.from_pretrained(out_dir)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name if strategy == 'prompt' else out_dir)\n    model.to(device).eval()\n\n    preds, refs = [], []\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Evaluating {suffix}\"):\n        row_id = row['id'] if 'id' in row.index else int(row.name)\n        inputs = tokenizer(row['question'], row['context'], max_length=max_len, truncation=True,\n                           padding='max_length', return_tensors='pt').to(device)\n        with torch.no_grad(): out = model(**inputs)\n        s, e = out.start_logits.argmax(dim=-1).item(), out.end_logits.argmax(dim=-1).item()\n        pred = tokenizer.decode(inputs['input_ids'][0, s:e+1], skip_special_tokens=True)\n        preds.append({'id': row_id, 'prediction_text': pred})\n        refs.append({'id': row_id, 'answers': {'text': [row['answer_text']], 'answer_start': [row['answer_start']]}})\n\n    results = metric.compute(predictions=preds, references=refs)\n    print(f\"{suffix} results: {results}\")\n    return results\n\nimport evaluate\nmetric = evaluate.load(\"squad\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:22.818940Z","iopub.execute_input":"2025-05-29T18:52:22.819110Z","iopub.status.idle":"2025-05-29T18:52:23.484737Z","shell.execute_reply.started":"2025-05-29T18:52:22.819096Z","shell.execute_reply":"2025-05-29T18:52:23.484168Z"},"id":"2-we8VoXX-bj","outputId":"bd7144ab-1991-496f-a41e-eef35f1fdb92","colab":{"referenced_widgets":["e7e1b62e8ea54531896610b1743cfe11","306e56e1885548adb4c51f0838caa5dc"]}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# 3. `BERT` Model","metadata":{"id":"CxUvoCn6X-bj"}},{"cell_type":"markdown","source":"## 3.1 Download model","metadata":{"id":"pOmvu9ilX-bj"}},{"cell_type":"code","source":"model_name = 'bert-base-uncased'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:23.485630Z","iopub.execute_input":"2025-05-29T18:52:23.486063Z","iopub.status.idle":"2025-05-29T18:52:23.489432Z","shell.execute_reply.started":"2025-05-29T18:52:23.486035Z","shell.execute_reply":"2025-05-29T18:52:23.488847Z"},"id":"6Y2LeEnaX-bj"},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## 3.2 LoRA tuning","metadata":{"id":"2WcBhwypX-bj"}},{"cell_type":"markdown","source":"","metadata":{"id":"YW9pC2HKX-bj"}},{"cell_type":"code","source":"# 1) Prepare model\nmodel, tokenizer = prepare_model(model_name, 'lora')\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'lora', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:23.490286Z","iopub.execute_input":"2025-05-29T18:52:23.491050Z","iopub.status.idle":"2025-05-29T18:52:36.874270Z","shell.execute_reply.started":"2025-05-29T18:52:23.491025Z","shell.execute_reply":"2025-05-29T18:52:36.873497Z"},"id":"6TcssJ24X-bj","outputId":"dfc37932-b699-43ef-bab1-8d6e42359bf2","colab":{"referenced_widgets":["d20581ea4e4f4d8291a34c54c108d5ca","c36833cf290d46d5b7b7065ce92ad166","e51b996ef0f6408c8a854a721cf9a72d","0002eda1f67148b6b9db004593a548e5","c7cd0bba0aec491f9a500263c805fd4e"]}},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 5.7621\nSaved best checkpoint to best_bert-base-uncased_lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:04<00:00, 49.31it/s]","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 0.0, 'f1': 6.381351365989014}\nEpoch 1 — Validation → EM: 0.00%, F1: 6.38%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## 3.3 BitFit Tuning","metadata":{"id":"L0PcgwfJX-bj"}},{"cell_type":"code","source":"model, tokenizer = prepare_model(model_name, 'bitfit')\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'bitfit', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:36.877765Z","iopub.execute_input":"2025-05-29T18:52:36.877994Z","iopub.status.idle":"2025-05-29T18:52:50.457658Z","shell.execute_reply.started":"2025-05-29T18:52:36.877976Z","shell.execute_reply":"2025-05-29T18:52:50.456888Z"},"id":"T5B9SUK0X-bk","outputId":"6e613afc-43f6-4077-bfe0-bff0fed32526"},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 5.9259\nSaved best checkpoint to best_bert-base-uncased_bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:03<00:00, 59.10it/s]","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 0.0, 'f1': 2.1911436037154686}\nEpoch 1 — Validation → EM: 0.00%, F1: 2.19%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## 3.4 Prompt Tuning","metadata":{"id":"jXUeqhHbX-bk"}},{"cell_type":"code","source":"model, tokenizer = prepare_model(model_name, 'prompt')\n# print_prompt_tunable_params(model)\n\n# total training steps\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'prompt', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:52:50.459502Z","iopub.execute_input":"2025-05-29T18:52:50.459689Z","iopub.status.idle":"2025-05-29T18:53:05.864155Z","shell.execute_reply.started":"2025-05-29T18:52:50.459674Z","shell.execute_reply":"2025-05-29T18:53:05.863503Z"},"id":"zpFxMMU4X-bk","outputId":"aaec948d-030a-4b64-e327-7b47e2ca0090"},"outputs":[{"name":"stdout","text":"125\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 6.1288\nSaved best checkpoint to best_bert-base-uncased_prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:04<00:00, 49.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 2.6717809426750723}\nEpoch 1 — Validation → EM: 0.00%, F1: 2.67%\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"\n# model, tokenizer = prepare_model(model_name, 'prompt')\n# print_prompt_tunable_params(model)\n# # total training steps\n# loader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n# LEARNING_RATE = 2e-4\n# train_and_save(model, tokenizer, loader, 'prompt', model_name, val_df)\n# LEARNING_RATE = 2e-4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:53:05.865670Z","iopub.execute_input":"2025-05-29T18:53:05.865913Z","iopub.status.idle":"2025-05-29T18:53:05.869413Z","shell.execute_reply.started":"2025-05-29T18:53:05.865891Z","shell.execute_reply":"2025-05-29T18:53:05.868643Z"},"id":"4PUgmOE0X-bk"},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## 3.5 Evaluation","metadata":{"id":"OpzHSf0UX-bk"}},{"cell_type":"code","source":"for strat in ['lora', 'bitfit', 'prompt']:\n    print(f\"Evaluating strategy: {strat or 'full'}\")\n    evaluate_model(model_name, strat, val_df, metric, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:53:05.870168Z","iopub.execute_input":"2025-05-29T18:53:05.870401Z","iopub.status.idle":"2025-05-29T18:53:18.653207Z","shell.execute_reply.started":"2025-05-29T18:53:05.870378Z","shell.execute_reply":"2025-05-29T18:53:18.652615Z"},"id":"dCdb-Ix4X-bk","outputId":"9f926eaa-252b-4197-c1cb-d54dd7219f30"},"outputs":[{"name":"stdout","text":"Evaluating strategy: lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:04<00:00, 49.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 0.0, 'f1': 6.381351365989014}\nEvaluating strategy: bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:03<00:00, 59.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 0.0, 'f1': 2.1911436037154686}\nEvaluating strategy: prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:03<00:00, 50.27it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 1.0006697614835673}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# 4. `RoBERTa` Model","metadata":{"id":"KNRt75HMX-bk"}},{"cell_type":"markdown","source":" ## 4.1 Download model","metadata":{"id":"KreqRapUX-bk"}},{"cell_type":"code","source":"model_name = 'roberta-base'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:53:18.654561Z","iopub.execute_input":"2025-05-29T18:53:18.654821Z","iopub.status.idle":"2025-05-29T18:53:18.658409Z","shell.execute_reply.started":"2025-05-29T18:53:18.654801Z","shell.execute_reply":"2025-05-29T18:53:18.657678Z"},"id":"LDlPEOQRX-bk"},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":" ## 4.2 LoRA tuning","metadata":{"id":"Z5_1bVK8X-bk"}},{"cell_type":"code","source":"model, tokenizer = prepare_model(model_name, 'lora')\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'lora', model_name, val_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:53:18.659125Z","iopub.execute_input":"2025-05-29T18:53:18.659451Z","iopub.status.idle":"2025-05-29T18:53:32.502107Z","shell.execute_reply.started":"2025-05-29T18:53:18.659434Z","shell.execute_reply":"2025-05-29T18:53:32.501326Z"},"id":"J4NdojB-X-bk","outputId":"cc880f1d-1ea0-4b90-f260-149c80941e04","colab":{"referenced_widgets":["3a09ba0fc2bb4ab7a5a572a6263030c3","a86265e967024488aab7539854bfcc1c","308888411d7c4ad088e0297eecd6c864","91f4dfd3fe8d42cf80b0f657c68589e4","6344b7dcef63430b9e12a60061fb146a","0af6d23ca865450b94094bf7e9119d0b"]}},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 5.7536\nSaved best checkpoint to best_roberta-base_lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:04<00:00, 48.70it/s]","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 1.0, 'f1': 5.469690691201836}\nEpoch 1 — Validation → EM: 1.00%, F1: 5.47%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 4.3 BitFit Tuning","metadata":{"id":"xf0hcMb2X-bl"}},{"cell_type":"code","source":"model, tokenizer = prepare_model(model_name, 'bitfit')\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'bitfit', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:53:32.504006Z","iopub.execute_input":"2025-05-29T18:53:32.504230Z","iopub.status.idle":"2025-05-29T18:53:45.718416Z","shell.execute_reply.started":"2025-05-29T18:53:32.504212Z","shell.execute_reply":"2025-05-29T18:53:45.717686Z"},"id":"D6Zq551EX-bl","outputId":"4393636f-ad23-4b45-cc02-5f73b305339e"},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 5.8349\nSaved best checkpoint to best_roberta-base_bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:03<00:00, 58.61it/s]","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 1.0, 'f1': 5.268477570432082}\nEpoch 1 — Validation → EM: 1.00%, F1: 5.27%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## 4.4 Prompt Tuning","metadata":{"id":"gHmXYHHPX-bl"}},{"cell_type":"code","source":"model, tokenizer = prepare_model(model_name, 'prompt')\n# print_prompt_tunable_params(model)\n\n# total training steps\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'prompt', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:53:45.719905Z","iopub.execute_input":"2025-05-29T18:53:45.720086Z","iopub.status.idle":"2025-05-29T18:54:01.572429Z","shell.execute_reply.started":"2025-05-29T18:53:45.720072Z","shell.execute_reply":"2025-05-29T18:54:01.571748Z"},"id":"cWMpSGcfX-bl","outputId":"f252bb31-62a0-454e-cde3-6420840d7daf"},"outputs":[{"name":"stdout","text":"126\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 6.2733\nSaved best checkpoint to best_roberta-base_prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:03<00:00, 50.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 0.09090909090909091}\nEpoch 1 — Validation → EM: 0.00%, F1: 0.09%\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## 4.5 Evaluation","metadata":{"id":"-uoM6kUQX-bp"}},{"cell_type":"code","source":"for strat in ['lora', 'bitfit', 'prompt']:\n    print(f\"Evaluating strategy: {strat or 'full'}\")\n    evaluate_model(model_name, strat, val_df, metric, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:01.573998Z","iopub.execute_input":"2025-05-29T18:54:01.574236Z","iopub.status.idle":"2025-05-29T18:54:14.762181Z","shell.execute_reply.started":"2025-05-29T18:54:01.574217Z","shell.execute_reply":"2025-05-29T18:54:14.761395Z"},"id":"JmRS3kXPX-bp","outputId":"14348449-544c-41da-c70a-3bdeb3ed7b7d"},"outputs":[{"name":"stdout","text":"Evaluating strategy: lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:04<00:00, 48.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 1.0, 'f1': 5.469690691201836}\nEvaluating strategy: bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:03<00:00, 58.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 1.0, 'f1': 5.268477570432082}\nEvaluating strategy: prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:03<00:00, 50.57it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 0.17669172932330826}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"# 5. `distll-bert` Model","metadata":{"id":"AA3COw0uX-bp"}},{"cell_type":"markdown","source":"## 5.1 Download model (call prepare_model)","metadata":{"id":"rchX0dykX-bp"}},{"cell_type":"code","source":"model_name = \"distilbert-base-uncased\"\ntokenizer     = AutoTokenizer.from_pretrained(model_name)\ntrain_ds      = QADataset(train_df, tokenizer, max_length=384)\ntrain_loader  = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\ntrain_steps = len(train_loader) * EPOCHS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:14.763419Z","iopub.execute_input":"2025-05-29T18:54:14.763642Z","iopub.status.idle":"2025-05-29T18:54:15.019060Z","shell.execute_reply.started":"2025-05-29T18:54:14.763624Z","shell.execute_reply":"2025-05-29T18:54:15.018467Z"},"id":"7LUNxya8X-bp","outputId":"9f7aff8e-99b8-439d-9d9b-79e50508acf5","colab":{"referenced_widgets":["9f27dcedcbe34ee18b2789f490cd86e7","ce16d2b2bdb846ba9d89f454fbfa258c","9c4886932a5c45a0b04ebdd746adde63","d23e4f8af7094746b3707203db0def19"]}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":" ## 5.2 LoRA tuning","metadata":{"id":"muDYC1EWX-bp"}},{"cell_type":"code","source":"model, tokenizer = prepare_model(model_name, 'lora')\nloader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\ntrain_and_save(model, tokenizer, loader, 'lora', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:15.020397Z","iopub.execute_input":"2025-05-29T18:54:15.020666Z","iopub.status.idle":"2025-05-29T18:54:22.621180Z","shell.execute_reply.started":"2025-05-29T18:54:15.020648Z","shell.execute_reply":"2025-05-29T18:54:22.620381Z"},"id":"EWeKxB62X-bp","outputId":"82a56872-772a-403f-b548-208efa766058","colab":{"referenced_widgets":["21ddbc44b04e4ec288c0e464b42e7346"]}},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 5.7967\nSaved best checkpoint to best_distilbert-base-uncased_lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:02<00:00, 85.41it/s]","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 0.0, 'f1': 4.40384368797905}\nEpoch 1 — Validation → EM: 0.00%, F1: 4.40%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## 5.3 BitFit Tuning","metadata":{"id":"mMj_9VxyX-bq"}},{"cell_type":"code","source":"  model, tokenizer = prepare_model(model_name, 'bitfit')\n  loader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n  train_and_save(model, tokenizer, loader, 'bitfit', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:22.623368Z","iopub.execute_input":"2025-05-29T18:54:22.623576Z","iopub.status.idle":"2025-05-29T18:54:29.622322Z","shell.execute_reply.started":"2025-05-29T18:54:22.623561Z","shell.execute_reply":"2025-05-29T18:54:29.621714Z"},"id":"romHrwCnX-bq","outputId":"1af1b13f-759c-434a-a5ae-912b09d39273"},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 5.9480\nSaved best checkpoint to best_distilbert-base-uncased_bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:01<00:00, 107.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 0.0, 'f1': 3.63547762844776}\nEpoch 1 — Validation → EM: 0.00%, F1: 3.64%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## 5.4 Prompt Tuning","metadata":{"id":"UYFh3zBsX-bq"}},{"cell_type":"code","source":" model, tokenizer = prepare_model(model_name, 'prompt')\n loader = DataLoader(QADataset(train_df, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n train_and_save(model, tokenizer, loader, 'prompt', model_name, val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:29.623661Z","iopub.execute_input":"2025-05-29T18:54:29.623851Z","iopub.status.idle":"2025-05-29T18:54:38.225254Z","shell.execute_reply.started":"2025-05-29T18:54:29.623836Z","shell.execute_reply":"2025-05-29T18:54:38.224488Z"},"id":"egPw7yHBX-bq","outputId":"26794864-0242-4614-d567-f683f7f9295e"},"outputs":[{"name":"stdout","text":"125\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average loss: 6.2105\nSaved best checkpoint to best_distilbert-base-uncased_prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:02<00:00, 91.43it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 0.0}\nEpoch 1 — Validation → EM: 0.00%, F1: 0.00%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 5.5 Evaluation","metadata":{"id":"A37MZ7e-X-bq"}},{"cell_type":"code","source":"for strat in ['lora', 'bitfit', 'prompt']:\n    print(f\"Evaluating MiniLM strategy: {strat or 'full'}\")\n    evaluate_model(model_name, strat, val_df, metric, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:38.226799Z","iopub.execute_input":"2025-05-29T18:54:38.227037Z","iopub.status.idle":"2025-05-29T18:54:45.623487Z","shell.execute_reply.started":"2025-05-29T18:54:38.227018Z","shell.execute_reply":"2025-05-29T18:54:45.622716Z"},"id":"Eyhzfd5VX-bq","outputId":"ff555a84-b982-4084-ccdb-4f0e2b26cd48"},"outputs":[{"name":"stdout","text":"Evaluating MiniLM strategy: lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:02<00:00, 84.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 0.0, 'f1': 4.40384368797905}\nEvaluating MiniLM strategy: bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:01<00:00, 108.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 0.0, 'f1': 3.63547762844776}\nEvaluating MiniLM strategy: prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:02<00:00, 91.83it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 0.44537120920661943}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"# 6. Evaluating pretrain models","metadata":{"id":"QTrXk8oaX-bq"}},{"cell_type":"code","source":"\nqa_parameters_path = kagglehub.dataset_download('kiwiresting/qa-parameters')\ndef evaluate_pretrained_model(model_name, strategy, df, metric, device, max_len=MAX_LEN):\n    suffix = _suffix(strategy)\n    out_dir = qa_parameters_path + f\"/best_{model_name.replace('/', '_')}_{suffix}\"\n\n\n    if strategy == 'prompt':\n        # Load base model and adapter separately for prompt tuning\n        base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        config = PromptTuningConfig.from_pretrained(out_dir)\n        model = get_peft_model(base_model, config)\n    else:\n        # For full finetuning, bitfit, or LoRA\n        model = AutoModelForQuestionAnswering.from_pretrained(out_dir)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name if strategy == 'prompt' else out_dir)\n    model.to(device).eval()\n\n    preds, refs = [], []\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Evaluating {suffix}\"):\n        row_id = row['id'] if 'id' in row.index else int(row.name)\n        inputs = tokenizer(row['question'], row['context'], max_length=max_len, truncation=True,\n                           padding='max_length', return_tensors='pt').to(device)\n        with torch.no_grad(): out = model(**inputs)\n        s, e = out.start_logits.argmax(dim=-1).item(), out.end_logits.argmax(dim=-1).item()\n        pred = tokenizer.decode(inputs['input_ids'][0, s:e+1], skip_special_tokens=True)\n        preds.append({'id': row_id, 'prediction_text': pred})\n        refs.append({'id': row_id, 'answers': {'text': [row['answer_text']], 'answer_start': [row['answer_start']]}})\n\n    results = metric.compute(predictions=preds, references=refs)\n    print(f\"{suffix} results: {results}\")\n    return results\n\n\nqa_parameters_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:54:45.624769Z","iopub.execute_input":"2025-05-29T18:54:45.625061Z","iopub.status.idle":"2025-05-29T18:54:45.842434Z","shell.execute_reply.started":"2025-05-29T18:54:45.625043Z","shell.execute_reply":"2025-05-29T18:54:45.841853Z"},"id":"OJY0GOD6X-bq","outputId":"c8256201-a501-43a7-f8ea-94c70791eed0"},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/qa-parameters'"},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"## 6.1 BERT Model","metadata":{"id":"ZpMoTJwiX-bq"}},{"cell_type":"code","source":"model_name = 'bert-base-uncased'\nfor strat in ['lora', 'bitfit', 'prompt']:\n    print(f\"Evaluating strategy: {strat or 'full'}\")\n    evaluate_pretrained_model(model_name, strat, val_df, metric, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:56:03.264447Z","iopub.execute_input":"2025-05-29T18:56:03.265051Z","iopub.status.idle":"2025-05-29T18:56:20.251362Z","shell.execute_reply.started":"2025-05-29T18:56:03.265028Z","shell.execute_reply":"2025-05-29T18:56:20.250569Z"},"id":"rV1MhuGJX-br","outputId":"d1c83684-0bbe-443d-975d-85b0dfbcde00"},"outputs":[{"name":"stdout","text":"Evaluating strategy: lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:04<00:00, 48.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 66.0, 'f1': 77.02199875796069}\nEvaluating strategy: bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:03<00:00, 58.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 53.0, 'f1': 64.57645462178297}\nEvaluating strategy: prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:03<00:00, 50.13it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 2.56462783448008}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"## 6.2 RoBERTa Model","metadata":{"id":"LGyyotr9X-br"}},{"cell_type":"code","source":"model_name = 'roberta-base'\nfor strat in ['lora', 'bitfit', 'prompt']:\n    print(f\"Evaluating strategy: {strat or 'full'}\")\n    evaluate_pretrained_model(model_name, strat, val_df, metric, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:56:24.384354Z","iopub.execute_input":"2025-05-29T18:56:24.384887Z","iopub.status.idle":"2025-05-29T18:56:41.343852Z","shell.execute_reply.started":"2025-05-29T18:56:24.384844Z","shell.execute_reply":"2025-05-29T18:56:41.343035Z"},"id":"84KJfYPVX-br","outputId":"b32f1793-be95-4de4-cd90-b00ff0832fa1"},"outputs":[{"name":"stdout","text":"Evaluating strategy: lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:04<00:00, 48.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 86.0, 'f1': 89.30238095238093}\nEvaluating strategy: bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:03<00:00, 58.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 80.0, 'f1': 84.73809523809523}\nEvaluating strategy: prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:03<00:00, 50.24it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 0.0}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"## 6.3 `distll-bert` Model","metadata":{"id":"3G3-pd9qX-br"}},{"cell_type":"code","source":"model_name = \"distilbert-base-uncased\"\nfor strat in ['lora', 'bitfit', 'prompt']:\n    print(f\"Evaluating MiniLM strategy: {strat or 'full'}\")\n    evaluate_pretrained_model(model_name, strat, val_df, metric, device)","metadata":{"trusted":true,"id":"wbP9XrYAX-br","execution":{"iopub.status.busy":"2025-05-29T18:57:29.848141Z","iopub.execute_input":"2025-05-29T18:57:29.848446Z","iopub.status.idle":"2025-05-29T18:57:37.269086Z","shell.execute_reply.started":"2025-05-29T18:57:29.848425Z","shell.execute_reply":"2025-05-29T18:57:37.268345Z"}},"outputs":[{"name":"stdout","text":"Evaluating MiniLM strategy: lora\n","output_type":"stream"},{"name":"stderr","text":"Evaluating lora: 100%|██████████| 200/200 [00:02<00:00, 84.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"lora results: {'exact_match': 67.5, 'f1': 76.60000674968785}\nEvaluating MiniLM strategy: bitfit\n","output_type":"stream"},{"name":"stderr","text":"Evaluating bitfit: 100%|██████████| 200/200 [00:01<00:00, 107.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"bitfit results: {'exact_match': 48.0, 'f1': 59.92702455403676}\nEvaluating MiniLM strategy: prompt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating prompt: 100%|██████████| 200/200 [00:02<00:00, 91.74it/s]","output_type":"stream"},{"name":"stdout","text":"prompt results: {'exact_match': 0.0, 'f1': 0.0}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":56}]}